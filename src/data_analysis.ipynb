{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07a23f2",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e151273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da604a",
   "metadata": {},
   "source": [
    "# Get the Data Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_data_path = \"../Data/Contadores.xlsx\"\n",
    "cont = pd.read_excel(cont_data_path)\n",
    "cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cont[cont['calibre'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbef140",
   "metadata": {},
   "source": [
    "As we see, there is an anomalous **calibre** 0. Soon, we will see that these are not present on the **Telemetria** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetria_data_path = \"../Data/TelemetriaConsumosVilaSol_v2.csv\"\n",
    "tel = pd.read_csv(telemetria_data_path)\n",
    "tel['data'] = pd.to_datetime(tel['data'])\n",
    "tel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ee6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tel['contact_id'].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_count = 0\n",
    "for i in range(23):\n",
    "    col_name = f\"index_{i}\"\n",
    "    na_count += tel[col_name].isna().sum()\n",
    "\n",
    "na_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df96fc",
   "metadata": {},
   "source": [
    "## Process Telemetry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24af5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor():\n",
    "    def __init__(self, df1, df2):\n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "    \n",
    "    def change_col_names(self):\n",
    "        '''\n",
    "        Change column names from index_(num) to just num\n",
    "        '''\n",
    "        column_names = self.df1.columns.values # Get the column names\n",
    "        names_map = {} # Old -> New\n",
    "\n",
    "        for name in column_names:\n",
    "            if name.startswith('index'):\n",
    "                num = name.split('_')[1]\n",
    "                if len(num) == 1:\n",
    "                    num = '0' + num # If it is only a single digit\n",
    "                \n",
    "                names_map[name] = num\n",
    "\n",
    "        return self.df1.rename(columns=names_map, inplace=False)\n",
    "\n",
    "    def add_columns(self, df):\n",
    "        '''\n",
    "        Add info from self.df2 to self.df1 (calibre, ano_contador, tipo_consumo)\n",
    "        '''\n",
    "        new_df = df.merge(self.df2.drop(columns=[\"calibre\"]), on=['contact_id'], how='left')\n",
    "        cont = list(set(new_df.loc[new_df[\"ano_contador\"].isna()][\"contact_id\"]))\n",
    "        if cont:\n",
    "            print(f\"The following {len(cont)} contadores didn't have matching information available on the other table:\")\n",
    "            print(f\"{', '.join(cont)}\")\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "    def get_differences(self, df):\n",
    "        \"\"\"\n",
    "        Get differences between consecutive values\n",
    "        \"\"\"\n",
    "        hour_cols = [f'{i:02d}' for i in range(24)]\n",
    "\n",
    "        # Sort by contact_id and date to ensure proper ordering\n",
    "        df = df.sort_values(['contact_id', 'data']).reset_index(drop=True)\n",
    "\n",
    "        # Get previous day's information for the same contact_id\n",
    "        grouped_df = df.groupby(['contact_id', 'id'])\n",
    "\n",
    "        prev_contact_id = grouped_df['contact_id'].shift(1)\n",
    "        prev_id = grouped_df['id'].shift(1)\n",
    "        prev_day = grouped_df['data'].shift(1)\n",
    "        prev_day_23 = grouped_df['23'].shift(1)\n",
    "\n",
    "        # Calculate the expected previous date (current date - 1 day)\n",
    "        expected_prev_date = df['data'] - pd.Timedelta(days=1)\n",
    "        \n",
    "        # Only compute if the id right before is the same and only 1 day difference\n",
    "        valid_prev_day = (df['id'] == prev_id) & (df['contact_id'] == prev_contact_id) & (prev_day == expected_prev_date)\n",
    "        \n",
    "        df['consumption_00'] = np.where(valid_prev_day, \n",
    "                                        df['00'] - prev_day_23, \n",
    "                                        np.nan)\n",
    "\n",
    "        # Create new columns for hourly consumption (hours 1-23)\n",
    "        for i in range(1, 24):\n",
    "            current_hour = hour_cols[i]\n",
    "            previous_hour = hour_cols[i - 1]\n",
    "            df[f'consumption_{current_hour}'] = df[current_hour] - df[previous_hour]\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_duplicate_lc_on_same_date(self, df):\n",
    "        \"\"\"\n",
    "        Identify contact_ids (LCs) that have multiple different ids on the same date.\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by contact_id and data, count unique ids\n",
    "        lc_date_id_counts = df.groupby(['contact_id', 'data'])['id'].nunique().reset_index()\n",
    "        lc_date_id_counts.columns = ['contact_id', 'data', 'num_ids']\n",
    "        \n",
    "        # Find contact_ids that have more than 1 id on the same date\n",
    "        problematic_lc_dates = lc_date_id_counts[lc_date_id_counts['num_ids'] > 1]\n",
    "        \n",
    "        # Get unique contact_ids that have this problem\n",
    "        problematic_lcs = problematic_lc_dates['contact_id'].unique()\n",
    "        \n",
    "        if len(problematic_lcs) > 0:\n",
    "            print(f\"The following {len(problematic_lcs)} LC had repeated contadores:\")\n",
    "            print(\", \".join(problematic_lcs))\n",
    "            df_cleaned = df[~df['contact_id'].isin(problematic_lcs)].copy()\n",
    "        else:\n",
    "            df_cleaned = df.copy()\n",
    "\n",
    "        \n",
    "        return df_cleaned\n",
    "\n",
    "\n",
    "    def correct_cumulative_values(self, df):\n",
    "        \"\"\"\n",
    "        Create cumulative values decreasing\n",
    "        \"\"\"\n",
    "        df_corrected = df.copy()\n",
    "    \n",
    "        hour_cols = [f'{i:02d}' for i in range(24)]\n",
    "        \n",
    "        # Sort by contact_id, id, and date to ensure proper chronological ordering\n",
    "        df_corrected = df_corrected.sort_values(['contact_id', 'data']).reset_index(drop=True)\n",
    "        \n",
    "        # Track negative consumption occurrences\n",
    "        negative_records = []\n",
    "        \n",
    "        # Process each contact_id and id combination separately\n",
    "        for (contact_id, meter_id), group in df_corrected.groupby(['contact_id', 'id']):\n",
    "            indices = group.index.tolist()\n",
    "            \n",
    "            # Keep track of the last valid value across dates\n",
    "            last_value = None\n",
    "            \n",
    "            # Process each date sequentially\n",
    "            for idx in indices:\n",
    "                # Process each hour sequentially for this date\n",
    "                for hour in hour_cols:\n",
    "                    current_value = df_corrected.loc[idx, hour]\n",
    "                    \n",
    "                    if pd.isna(current_value):\n",
    "                        continue\n",
    "\n",
    "                    # Compare with last_value \n",
    "                    if last_value is not None and current_value < last_value:\n",
    "                        negative_consumption = current_value - last_value\n",
    "                        negative_records.append(negative_consumption)\n",
    "                        df_corrected.loc[idx, hour] = last_value\n",
    "        \n",
    "                    else:\n",
    "                        last_value = current_value  # Update to current value\n",
    "        \n",
    "        return df_corrected, negative_records\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "        '''\n",
    "        Process everything at once\n",
    "        '''\n",
    "\n",
    "        print(\"Changing column names...\")\n",
    "        df = self.change_col_names()\n",
    "\n",
    "        print(\"\\nMerging both datasets to get 'tipo_consumo' and 'ano_contador'...\")\n",
    "        merged_df = self.add_columns(df)\n",
    "\n",
    "        print(\"\\nRemoving LC that have more than 1 contador at the same time\")\n",
    "        merged_df = self.remove_duplicate_lc_on_same_date(merged_df)\n",
    "\n",
    "        print(\"\\nFixing cumulative values...\")\n",
    "        merged_df, negative_records = self.correct_cumulative_values(merged_df)\n",
    "\n",
    "        print(\"\\nGetting hourly consumptions...\")\n",
    "        merged_df = self.get_differences(merged_df)\n",
    "        \n",
    "        return merged_df, negative_records\n",
    "\n",
    "\n",
    "proc = Processor(df1=tel, df2=cont)\n",
    "proc_tel, neg_records = proc.process()\n",
    "\n",
    "# Save new table\n",
    "output_path = \"../Data/Processed_Telemetria.csv\"\n",
    "proc_tel.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "proc_tel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5ca62",
   "metadata": {},
   "source": [
    "Let's obtain some statistics regarding the negative consumptions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e00424",
   "metadata": {},
   "outputs": [],
   "source": [
    "if neg_records:\n",
    "    negative_series = pd.Series(neg_records)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"NEGATIVE CONSUMPTION STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTotal negative occurrences: {len(neg_records)}\")\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(negative_series.describe())\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No negative consumption values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [f'{i:02d}' for i in range(24)]:\n",
    "    column_name = \"consumption_\" + column \n",
    "    print(f\"Number of negatives on column {column_name}: {(proc_tel[column_name] < 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(set(proc_tel['calibre'])))) # Print sorted calibre values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61a4f2",
   "metadata": {},
   "source": [
    "Get all the anomalies that have lower value on hour 23 than on hour 22 (this happens in other hours as well, but in this one is more frequent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b5cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = proc_tel.loc[proc_tel['consumption_23'] < 0, ['id', 'contact_id', 'calibre', 'tipo_consumo', 'data', '22', '23']]\n",
    "#df.to_csv(\"../Data/Anomalies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb2d8a",
   "metadata": {},
   "source": [
    "## Group time series by pair (calibre, tipo_consumo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b77e14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"../Data/Processed_Telemetria.csv\"\n",
    "proc_tel = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacdb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tel[proc_tel['contact_id'] == 'LC43022']['calibre'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1320389",
   "metadata": {},
   "source": [
    "Since 'Serviços 2º contador-Rega Condominio' (20) has **very low** representability, we chose to remove since the analysis would never lead to good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "111cbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map pair (calibre, tipo_consumo) to index in future list\n",
    "df_list = []\n",
    "pair2idx = {}\n",
    "idx2pair = []\n",
    "temp = 0\n",
    "\n",
    "for idx, ((calibre, consumo), group) in enumerate(proc_tel.groupby([\"calibre\", \"tipo_consumo\"])):\n",
    "    consumo = consumo.strip() # Remove empty space on the right\n",
    "    if int(calibre) == 20 and consumo == 'Serviços 2º contador-Rega Condominio': # Ignore VERY low representability\n",
    "        temp = 1\n",
    "        continue\n",
    "    \n",
    "    pair2idx[(int(calibre), consumo)] = idx - temp\n",
    "    idx2pair.append((int(calibre), consumo))\n",
    "    df_list.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2feb59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that it works\n",
    "idx = pair2idx[(20, \"Doméstico\")]\n",
    "df_list[idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b35c85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lengths of each subgroup\n",
    "lista = []\n",
    "\n",
    "for key in pair2idx.keys():\n",
    "    idx = pair2idx[key]\n",
    "    lista.append([len(df_list[idx]), key[0], key[1]]) # (length, calibre, tipo_consumo)    \n",
    "\n",
    "lista.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc833b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting\n",
    "def plot(results):\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    bars = plt.barh(results[\"tipo_consumo\"] + \" (\" + results[\"calibre\"].astype(str) + \")\", results[\"length\"])\n",
    "    plt.xlabel(\"Number of recorded days\")\n",
    "    plt.ylabel(\"Subgroup - tipo_consumo (calibre)\")\n",
    "    plt.title(\"Recorded days per subgroup\")\n",
    "\n",
    "    # Add numbers on bars\n",
    "    for bar, value in zip(bars, results[\"length\"]):\n",
    "        plt.text(\n",
    "            value,                             # x-position (at end of bar)\n",
    "            bar.get_y() + bar.get_height()/2,  # y-position (center of bar)\n",
    "            str(value),                        # text = length\n",
    "            va=\"center\", ha=\"left\", fontsize=8 # Small changes to improve\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "results = pd.DataFrame(lista, columns=[\"length\", \"calibre\", \"tipo_consumo\"])\n",
    "plot(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b9d1d",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot_dataframes(df_list, group_names):\n",
    "    \"\"\"\n",
    "    Process dataframes and create boxplot analyses for all groups in one figure.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_groups = len(df_list)\n",
    "    df_list_processed = []\n",
    "    \n",
    "    # Create a large figure with subplots: rows = groups, cols = 3 (month, day, hour)\n",
    "    fig, axes = plt.subplots(n_groups, 3, figsize=(30, 5 * n_groups))\n",
    "    \n",
    "    \n",
    "    fig.suptitle('Consumption Analysis - All Groups', fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    for idx, (df, (calibre, tipo_consumo)) in enumerate(zip(df_list, group_names)):\n",
    "        \n",
    "        # Process the dataframe\n",
    "        df_processed = process_dataframe(df)\n",
    "        df_list_processed.append(df_processed)\n",
    "        \n",
    "        # Create the three boxplot analyses for this group\n",
    "        create_boxplots_row(df_processed, calibre, tipo_consumo, axes[idx])\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 0.99]) # Leave space for title at top\n",
    "    plt.show()\n",
    "\n",
    "    return df_list_processed\n",
    "\n",
    "\n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Transform wide format (date rows, hour columns) to long format with \n",
    "    separate month, day, and hour columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get Date column name\n",
    "    date_col = 'data'\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Get hour columns \n",
    "    hour_cols = [col for col in df.columns if col.isdigit()]\n",
    "    conshour_cols = [col for col in df.columns if col.startswith('consumption_')]\n",
    "    \n",
    "    # Melt the dataframe to long format\n",
    "    df_long_values = df.melt(\n",
    "    id_vars=[col for col in df.columns if col not in hour_cols and col not in conshour_cols],\n",
    "    value_vars=hour_cols,\n",
    "    var_name='hour',\n",
    "    value_name='cumulative_value'\n",
    "    )\n",
    "\n",
    "    # Melt the consumption columns\n",
    "    df_long_consumption = df.melt(\n",
    "        id_vars=[col for col in df.columns if col not in hour_cols and not col.startswith('consumption_')],\n",
    "        value_vars= conshour_cols,\n",
    "        var_name='hour',\n",
    "        value_name='consumption'\n",
    "    )\n",
    "\n",
    "    # Extract the hour from consumption column names (remove 'consumption_' prefix)\n",
    "    df_long_consumption['hour'] = df_long_consumption['hour'].str.replace('consumption_', '')\n",
    "\n",
    "    # Merge on all id_vars + 'hour'\n",
    "    id_cols = [col for col in df.columns if col not in hour_cols and col not in conshour_cols]\n",
    "    df_long = df_long_values.merge(\n",
    "        df_long_consumption,\n",
    "        on=id_cols + ['hour']\n",
    "    )\n",
    "    \n",
    "    # Extract month, day of week, and convert hour to integer\n",
    "    df_long['month'] = df_long[date_col].dt.month\n",
    "    df_long['month_name'] = df_long[date_col].dt.month_name().apply(lambda x: x[:3])\n",
    "    df_long['day_of_week'] = df_long[date_col].dt.dayofweek\n",
    "    df_long['day_name'] = df_long[date_col].dt.day_name().apply(lambda x: x[:3])\n",
    "    df_long['hour'] = df_long['hour'].astype(int)\n",
    "    \n",
    "    return df_long\n",
    "\n",
    "\n",
    "def create_boxplots_row(df, calibre, tipo_consumo, axes_row):\n",
    "    \"\"\"\n",
    "    Create three boxplots in a single row for a processed dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    x_fontsize = y_fontsize = 15\n",
    "    title_fontsize = 17\n",
    "\n",
    "    # Remove any NaN values\n",
    "    df = df.dropna(subset=['consumption'])\n",
    "\n",
    "    # Add row label\n",
    "    axes_row[0].text(-0.15, 0.5, f'Calibre: {calibre}\\nTipo: {tipo_consumo}', \n",
    "                     transform=axes_row[0].transAxes, fontsize=20, \n",
    "                     fontweight='bold', va='center', ha='right')\n",
    "    \n",
    "    # Boxplot by Month\n",
    "    month_order = sorted(df['month'].unique())\n",
    "    month_labels = [df[df['month'] == m]['month_name'].iloc[0] for m in month_order]\n",
    "    data_by_month = [df[df['month'] == m]['consumption'].values for m in month_order]\n",
    "    \n",
    "    axes_row[0].boxplot(data_by_month, tick_labels=month_labels)\n",
    "    axes_row[0].set_xlabel('Month', fontweight='bold', fontsize=x_fontsize)\n",
    "    axes_row[0].set_ylabel('Consumption Value', fontweight='bold', fontsize=y_fontsize)\n",
    "    axes_row[0].set_title('By Month', fontsize=title_fontsize)\n",
    "    axes_row[0].tick_params(axis='x', rotation=45, labelsize=11)\n",
    "    axes_row[0].tick_params(axis='y', labelsize=11)\n",
    "    axes_row[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Boxplot by Day of Week\n",
    "    day_order = list(range(7))  # 0=Monday, 6=Sunday\n",
    "    day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    data_by_day = [df[df['day_of_week'] == d]['consumption'].values \n",
    "                   for d in day_order if d in df['day_of_week'].values]\n",
    "    valid_day_labels = [day_labels[d] for d in day_order if d in df['day_of_week'].values]\n",
    "    \n",
    "    axes_row[1].boxplot(data_by_day, tick_labels=valid_day_labels)\n",
    "    axes_row[1].set_xlabel('Day of Week', fontweight='bold', fontsize=x_fontsize)\n",
    "    axes_row[1].set_ylabel('Consumption Value', fontweight='bold', fontsize=y_fontsize)\n",
    "    axes_row[1].set_title('By Day of Week', fontsize=title_fontsize)\n",
    "    axes_row[1].tick_params(axis='x', labelsize=11)\n",
    "    axes_row[1].tick_params(axis='y', labelsize=11)\n",
    "    axes_row[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Boxplot by Hour\n",
    "    hour_order = sorted(df['hour'].unique())\n",
    "    data_by_hour = [df[df['hour'] == h]['consumption'].values for h in hour_order]\n",
    "    \n",
    "    axes_row[2].boxplot(data_by_hour, tick_labels=[f'{h:02d}' for h in hour_order])\n",
    "    axes_row[2].set_xlabel('Hour', fontweight='bold', fontsize=x_fontsize)\n",
    "    axes_row[2].set_ylabel('Consumption Value', fontweight='bold', fontsize=y_fontsize)\n",
    "    axes_row[2].set_title('By Hour', fontsize=title_fontsize)\n",
    "    axes_row[2].tick_params(axis='x', rotation=45, labelsize=11)\n",
    "    axes_row[2].tick_params(axis='y', labelsize=11)\n",
    "    axes_row[2].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "df_processed = process_and_plot_dataframes(df_list=df_list, group_names=idx2pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99504972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, cophenet\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SubgroupIdentifier:\n",
    "    \"\"\"\n",
    "    Identify subgroups within time series data based on statistical similarity.\n",
    "    Uses distribution comparison methods to cluster similar time periods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, calibre, tipo_consumo):\n",
    "        self.df = df.dropna(subset=['consumption'])\n",
    "        self.calibre = calibre\n",
    "        self.tipo_consumo = tipo_consumo\n",
    "        \n",
    "    def compare_distributions(self, group1_data, group2_data, method='ks'):\n",
    "        \"\"\"\n",
    "        Compare two distributions using statistical tests.\n",
    "        \"\"\"\n",
    "        if len(group1_data) == 0 or len(group2_data) == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        if method == 'ks':\n",
    "            statistic, _ = stats.ks_2samp(group1_data, group2_data)\n",
    "            return statistic\n",
    "            \n",
    "                \n",
    "        elif method == 'wasserstein':\n",
    "            dist = stats.wasserstein_distance(group1_data, group2_data)\n",
    "            data_range = max(group1_data.max(), group2_data.max()) - \\\n",
    "                        min(group1_data.min(), group2_data.min())\n",
    "            return dist / data_range if data_range > 0 else 0\n",
    "        \n",
    "        elif method == 'kl':\n",
    "            # Determine shared bin edges\n",
    "            min_val = min(group1_data.min(), group2_data.min())\n",
    "            max_val = max(group1_data.max(), group2_data.max())\n",
    "            \n",
    "            # Sturges Rule\n",
    "            n_bins = int(np.ceil(np.log2(len(group1_data) + len(group2_data)) + 1))\n",
    "            bins = np.linspace(min_val, max_val, n_bins + 1)\n",
    "            \n",
    "            # Create histograms\n",
    "            hist1, _ = np.histogram(group1_data, bins=bins)\n",
    "            hist2, _ = np.histogram(group2_data, bins=bins)\n",
    "            \n",
    "            # Convert to probability distributions \n",
    "            epsilon = 1e-10\n",
    "            p = (hist1 + epsilon) / (hist1.sum() + epsilon * n_bins)\n",
    "            q = (hist2 + epsilon) / (hist2.sum() + epsilon * n_bins)\n",
    "            \n",
    "            # Compute symmetric KL divergence\n",
    "            kl_pq = stats.entropy(p, q)\n",
    "            kl_qp = stats.entropy(q, p)\n",
    "            \n",
    "            return (kl_pq + kl_qp) / 2 # No correction needed for sufficiently small bin wideness\n",
    "    \n",
    "    def _compute_dunn_index(self, distance_matrix, labels):\n",
    "        \"\"\"\n",
    "        Compute Dunn Index (0, 1)\n",
    "        \"\"\"\n",
    "        n_clusters = len(np.unique(labels))\n",
    "        if n_clusters <= 1:\n",
    "            return 0\n",
    "        \n",
    "        # Compute inter-cluster distances (minimum distance between clusters)\n",
    "        min_inter_cluster = float('inf')\n",
    "        for i in range(n_clusters):\n",
    "            for j in range(i + 1, n_clusters):\n",
    "                cluster_i_indices = np.where(labels == i)[0]\n",
    "                cluster_j_indices = np.where(labels == j)[0]\n",
    "                \n",
    "                inter_dists = distance_matrix[np.ix_(cluster_i_indices, cluster_j_indices)] # Get every pairwise distance between points of cluster i and j\n",
    "                min_inter_cluster = min(min_inter_cluster, np.min(inter_dists))\n",
    "        \n",
    "        # Compute intra-cluster distances (maximum diameter within clusters)\n",
    "        max_intra_cluster = 0\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = np.where(labels == i)[0]\n",
    "            if len(cluster_indices) > 1:\n",
    "                intra_dists = distance_matrix[np.ix_(cluster_indices, cluster_indices)]\n",
    "                max_intra_cluster = max(max_intra_cluster, np.max(intra_dists))\n",
    "        \n",
    "        if max_intra_cluster == 0:\n",
    "            return 0\n",
    "        \n",
    "        return min_inter_cluster / max_intra_cluster\n",
    "    \n",
    "    def _compute_c_index(self, distance_matrix, labels):\n",
    "        \"\"\"\n",
    "        Compute C-Index (0, 1)\n",
    "        \"\"\"\n",
    "        n = len(labels)\n",
    "        n_clusters = len(np.unique(labels))\n",
    "        \n",
    "        if n_clusters <= 1 or n <= 1:\n",
    "            return 1\n",
    "        \n",
    "        # Get all pairwise distances\n",
    "        condensed_dist = squareform(distance_matrix)\n",
    "        all_distances = np.sort(condensed_dist)\n",
    "        \n",
    "        # Compute within-cluster distances\n",
    "        within_cluster_dists = []\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                if labels[i] == labels[j]:\n",
    "                    within_cluster_dists.append(distance_matrix[i, j])\n",
    "        \n",
    "        if len(within_cluster_dists) == 0:\n",
    "            return 1\n",
    "        \n",
    "        S_w = np.sum(within_cluster_dists)\n",
    "        n_w = len(within_cluster_dists)\n",
    "        \n",
    "        # Min and max possible sums\n",
    "        S_min = np.sum(all_distances[:n_w])\n",
    "        S_max = np.sum(all_distances[-n_w:])\n",
    "        \n",
    "        if S_max - S_min == 0:\n",
    "            return 0\n",
    "        \n",
    "        return (S_w - S_min) / (S_max - S_min)\n",
    "    \n",
    "    def _compute_silhouette(self, distance_matrix, labels):\n",
    "        \"\"\"\n",
    "        Compute Silhouette Coefficient (-1, 1)\n",
    "        \"\"\"\n",
    "        n_clusters = len(np.unique(labels))\n",
    "        if n_clusters <= 1 or n_clusters >= len(labels):\n",
    "            return -1\n",
    "        \n",
    "        try:\n",
    "            return silhouette_score(distance_matrix, labels, metric='precomputed')\n",
    "        except:\n",
    "            return -1\n",
    "    \n",
    "    def _evaluate_clustering(self, distance_matrix, labels):\n",
    "        \"\"\"\n",
    "        Evaluate clustering quality using multiple metrics.\n",
    "        Returns dict with all metrics.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'dunn': self._compute_dunn_index(distance_matrix, labels),\n",
    "            'c_index': self._compute_c_index(distance_matrix, labels),\n",
    "            'silhouette': self._compute_silhouette(distance_matrix, labels)\n",
    "        }\n",
    "    \n",
    "    def _find_optimal_clusters(self, linkage_matrix, max_clusters=5):\n",
    "        \"\"\"\n",
    "        Find optimal number of clusters using elbow method on dendrogram heights.\n",
    "        \"\"\"\n",
    "        if len(linkage_matrix) < 2:\n",
    "            return 1\n",
    "        \n",
    "        heights = linkage_matrix[:, 2]\n",
    "        if len(heights) < 3:\n",
    "            return 2\n",
    "        \n",
    "        diffs = np.diff(heights)\n",
    "        if len(diffs) > 0:\n",
    "            largest_jump_idx = np.argmax(diffs)\n",
    "            optimal_clusters = len(heights) - largest_jump_idx\n",
    "            return min(max(2, optimal_clusters), max_clusters)\n",
    "        \n",
    "        return 2\n",
    "    \n",
    "    def find_best_clustering_method(self, period_type='month', methods=['ks', 'wasserstein', 'kl'],\n",
    "                                   linkage_methods=['average', 'complete', 'single']):\n",
    "        \"\"\"\n",
    "        Test all combinations of distance metrics and linkage methods.\n",
    "        Select best based on validation indices ranking.\n",
    "        \"\"\"\n",
    "        col_map = {\n",
    "            'month': 'month_name',\n",
    "            'day': 'day_name',\n",
    "            'hour': 'hour'\n",
    "        }\n",
    "        col = col_map[period_type]\n",
    "        \n",
    "        periods = sorted(self.df[col].unique())\n",
    "        n_periods = len(periods)\n",
    "        \n",
    "        if n_periods <= 1:\n",
    "            return None, {'groups': [periods], 'dendrogram_data': None, 'distance_matrix': None}\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for method in methods:\n",
    "            # Create distance matrix\n",
    "            distance_matrix = np.zeros((n_periods, n_periods))\n",
    "            \n",
    "            for i, period1 in enumerate(periods):\n",
    "                for j, period2 in enumerate(periods):\n",
    "                    if i < j:\n",
    "                        data1 = self.df[self.df[col] == period1]['consumption'].values\n",
    "                        data2 = self.df[self.df[col] == period2]['consumption'].values\n",
    "                        \n",
    "                        dist = self.compare_distributions(data1, data2, method=method)\n",
    "                        if dist < 0:\n",
    "                            print(method)\n",
    "\n",
    "                        distance_matrix[i, j] = dist\n",
    "                        distance_matrix[j, i] = dist\n",
    "            \n",
    "            condensed_dist = squareform(distance_matrix)\n",
    "            \n",
    "            # Find best linkage method using cophenetic correlation\n",
    "            best_linkage_method = None\n",
    "            best_coph_corr = -1\n",
    "            best_linkage_matrix = None\n",
    "            \n",
    "            for link_method in linkage_methods: \n",
    "                try:                  \n",
    "                    link_matrix = linkage(condensed_dist, method=link_method)\n",
    "                    coph_corr, _ = cophenet(link_matrix, condensed_dist) # Needs new and old distances\n",
    "                    \n",
    "                    if coph_corr > best_coph_corr:\n",
    "                        best_coph_corr = coph_corr\n",
    "                        best_linkage_method = link_method\n",
    "                        best_linkage_matrix = link_matrix\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if best_linkage_matrix is None:\n",
    "                continue\n",
    "\n",
    "            # Find optimal number of clusters\n",
    "            n_clusters = self._find_optimal_clusters(best_linkage_matrix, \n",
    "                                                     max_clusters=min(5, n_periods))\n",
    "            \n",
    "            # Get cluster labels\n",
    "            clustering = AgglomerativeClustering(n_clusters=n_clusters,\n",
    "                                                linkage=best_linkage_method,\n",
    "                                                metric='precomputed')\n",
    "            \n",
    "            labels = clustering.fit_predict(distance_matrix)\n",
    "            \n",
    "            # Evaluate clustering\n",
    "            metrics = self._evaluate_clustering(distance_matrix, labels)\n",
    "            \n",
    "            results.append({\n",
    "                'method': method,\n",
    "                'linkage': best_linkage_method,\n",
    "                'n_clusters': n_clusters,\n",
    "                'coph_corr': best_coph_corr,\n",
    "                'dunn': metrics['dunn'],\n",
    "                'c_index': metrics['c_index'],\n",
    "                'silhouette': metrics['silhouette'],\n",
    "                'distance_matrix': distance_matrix,\n",
    "                'linkage_matrix': best_linkage_matrix,\n",
    "                'labels': labels\n",
    "            })\n",
    "        \n",
    "        if not results:\n",
    "            return None, {'groups': [periods], 'dendrogram_data': None, 'distance_matrix': None}\n",
    "        \n",
    "        # Rank each result by each metric\n",
    "        # Dunn: higher is better\n",
    "        # C-index: lower is better\n",
    "        # Silhouette: higher is better\n",
    "        dunn_sorted_indices = np.argsort([-r['dunn'] for r in results])\n",
    "        c_index_sorted_indices = np.argsort([r['c_index'] for r in results])\n",
    "        silhouette_sorted_indices = np.argsort([-r['silhouette'] for r in results])\n",
    "\n",
    "        # Convert sorted indices to ranks\n",
    "        dunn_ranks = np.empty(len(results), dtype=int)\n",
    "        dunn_ranks[dunn_sorted_indices] = np.arange(1, len(results) + 1)\n",
    "\n",
    "        c_index_ranks = np.empty(len(results), dtype=int)\n",
    "        c_index_ranks[c_index_sorted_indices] = np.arange(1, len(results) + 1)\n",
    "\n",
    "        silhouette_ranks = np.empty(len(results), dtype=int)\n",
    "        silhouette_ranks[silhouette_sorted_indices] = np.arange(1, len(results) + 1)\n",
    "        \n",
    "        # Compute mean rank\n",
    "        for i, result in enumerate(results):\n",
    "            result['mean_rank'] = np.mean([dunn_ranks[i], c_index_ranks[i], silhouette_ranks[i]])\n",
    "        \n",
    "        # Sort by mean rank, then by number of clusters\n",
    "        results.sort(key=lambda x: (x['mean_rank'], x['n_clusters']))\n",
    "        \n",
    "        best = results[0]\n",
    "        \n",
    "        # Group periods by cluster\n",
    "        groups = [[] for _ in range(best['n_clusters'])]\n",
    "        for period, label in zip(periods, best['labels']):\n",
    "            groups[label].append(str(period))\n",
    "        \n",
    "        groups = [sorted(g) for g in groups if g]\n",
    "        \n",
    "        result_dict = {\n",
    "            'groups': groups,\n",
    "            'dendrogram_data': best['linkage_matrix'],\n",
    "            'distance_matrix': best['distance_matrix'],\n",
    "            'period_labels': periods,\n",
    "            'method': best['method'],\n",
    "            'linkage': best['linkage'],\n",
    "            'n_clusters': best['n_clusters'],\n",
    "            'metrics': {\n",
    "                'dunn': round(best['dunn'], 3),\n",
    "                'c_index': round(best['c_index'], 3),\n",
    "                'silhouette': round(best['silhouette'], 3),\n",
    "                'cophenetic': round(best['coph_corr'], 3)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result_dict\n",
    "    \n",
    "    def plot_analysis(self, result, period_type='month'):\n",
    "        \"\"\"\n",
    "        Create visualization of clustering analysis with distribution boxplots.\n",
    "        \"\"\"\n",
    "        if result['dendrogram_data'] is None:\n",
    "            print(f\"Not enough {period_type}s to cluster\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "        \n",
    "        n_groups = len(result['groups'])\n",
    "        \n",
    "        # Prepare data for boxplot\n",
    "        match period_type:\n",
    "            case 'month':\n",
    "                order = sorted(self.df['month'].unique())\n",
    "                labels = [self.df[self.df['month'] == m]['month_name'].iloc[0] for m in order]\n",
    "                data = [self.df[self.df['month'] == m]['consumption'].values for m in order]\n",
    "            case 'day':\n",
    "                day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "                order = [d for d in range(7) if d in self.df['day_of_week'].values]\n",
    "                labels = [day_labels[d] for d in order]\n",
    "                data = [self.df[self.df['day_of_week'] == d]['consumption'].values for d in order]\n",
    "            case 'hour':\n",
    "                order = sorted(self.df['hour'].unique())\n",
    "                labels = [f'{h:02d}' for h in order]\n",
    "                data = [self.df[self.df['hour'] == h]['consumption'].values for h in order]\n",
    "        \n",
    "        # Plot boxplot\n",
    "        axes[0].boxplot(data, tick_labels=labels, patch_artist=True)\n",
    "        axes[0].set_xlabel(period_type.capitalize(), fontweight='bold', fontsize=13)\n",
    "        axes[0].set_ylabel('Consumption Value', fontweight='bold', fontsize=13)\n",
    "        axes[0].set_title(f'Distribution by {period_type.capitalize()}', fontsize=15)\n",
    "        axes[0].tick_params(axis='x', rotation=45, labelsize=11)\n",
    "        axes[0].tick_params(axis='y', labelsize=11)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot dendrogram\n",
    "        threshold = result['dendrogram_data'][-n_groups, 2] + 1e-10\n",
    "        dendrogram(result['dendrogram_data'], \n",
    "                  labels=result['period_labels'],\n",
    "                  ax=axes[1],\n",
    "                  color_threshold=threshold)\n",
    "        \n",
    "        axes[1].set_title(f'Hierarchical Clustering - {period_type.capitalize()}s', fontsize=15)\n",
    "        axes[1].set_xlabel(period_type.capitalize(), fontweight='bold', fontsize=13)\n",
    "        axes[1].set_ylabel('Distance', fontweight='bold', fontsize=13)\n",
    "        axes[1].tick_params(axis='x', rotation=45, labelsize=11)\n",
    "        axes[1].tick_params(axis='y', labelsize=11)\n",
    "        \n",
    "        # Plot distance matrix heatmap\n",
    "        im = axes[2].imshow(result['distance_matrix'], cmap='YlOrRd', aspect='auto')\n",
    "        axes[2].set_xticks(range(len(result['period_labels'])))\n",
    "        axes[2].set_yticks(range(len(result['period_labels'])))\n",
    "        axes[2].set_xticklabels(result['period_labels'], rotation=45, fontsize=11)\n",
    "        axes[2].set_yticklabels(result['period_labels'], fontsize=11)\n",
    "        axes[2].set_title(f'Distance Matrix - {period_type.capitalize()}s', fontsize=15)\n",
    "        plt.colorbar(im, ax=axes[2], label='Distance')\n",
    "        \n",
    "        # Add method info to title\n",
    "        method_info = f\"{result['method'].upper()}, {result['linkage']}, k={result['n_clusters']}\"\n",
    "        fig.suptitle(f'Clustering Analysis - {period_type.capitalize()} | {self.tipo_consumo} ({self.calibre})\\n{method_info}', \n",
    "                    fontsize=18, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nBest configuration: {result['method']} + {result['linkage']} (k={result['n_clusters']})\")\n",
    "        print(f\"Metrics: Dunn={result['metrics']['dunn']:.3f}, C-index={result['metrics']['c_index']:.3f}, \"\n",
    "              f\"Silhouette={result['metrics']['silhouette']:.3f}, Cophenetic={result['metrics']['cophenetic']:.3f}\")\n",
    "        print(f\"\\nIdentified {n_groups} subgroups:\")\n",
    "        for i, group in enumerate(result['groups'], 1):\n",
    "            print(f\"  Group {i}: {group}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def generate_json_config(self, day_result, month_result, hour_result):\n",
    "        \"\"\"\n",
    "        Generate complete JSON configuration for this group.\n",
    "        \"\"\"\n",
    "        hour_groups = []\n",
    "        for group in hour_result['groups']:\n",
    "            formatted_group = [f\"{int(h):02d}\" for h in group]\n",
    "            hour_groups.append(formatted_group)\n",
    "        \n",
    "        config = {\n",
    "            \"calibre\": str(self.calibre),\n",
    "            \"tipo_consumo\": self.tipo_consumo,\n",
    "            \"month_groups\": month_result['groups'],\n",
    "            \"day_groups\": day_result['groups'],\n",
    "            \"hour_groups\": hour_groups,\n",
    "            \"methods\": {\n",
    "                \"month\": {\n",
    "                    \"metric\": month_result['method'],\n",
    "                    \"linkage\": month_result['linkage'],\n",
    "                    \"n_clusters\": int(month_result['n_clusters']),\n",
    "                    \"quality\": month_result['metrics']\n",
    "                },\n",
    "                \"day\": {\n",
    "                    \"metric\": day_result['method'],\n",
    "                    \"linkage\": day_result['linkage'],\n",
    "                    \"n_clusters\": int(day_result['n_clusters']),\n",
    "                    \"quality\": day_result['metrics']\n",
    "                },\n",
    "                \"hour\": {\n",
    "                    \"metric\": hour_result['method'],\n",
    "                    \"linkage\": hour_result['linkage'],\n",
    "                    \"n_clusters\": int(hour_result['n_clusters']),\n",
    "                    \"quality\": hour_result['metrics']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return config\n",
    "\n",
    "\n",
    "def analyze_all_groups(df_list, idx2pair, output_path='../info/subgroups.json'):\n",
    "    \"\"\"\n",
    "    Analyze all groups with automatic method selection and generate combined JSON configuration.\n",
    "    \"\"\"\n",
    "    all_configs = []\n",
    "    \n",
    "    for idx, df in enumerate(df_list):\n",
    "        calibre, tipo_consumo = idx2pair[idx]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analyzing: {tipo_consumo} ({calibre})\")\n",
    "        print('='*60)\n",
    "        \n",
    "        identifier = SubgroupIdentifier(df, calibre, tipo_consumo)\n",
    "        \n",
    "        # Find best methods automatically\n",
    "        print(\"\\n--- MONTHS ---\")\n",
    "        month_result = identifier.find_best_clustering_method('month')\n",
    "        identifier.plot_analysis(month_result, 'month')\n",
    "        \n",
    "        print(\"\\n--- DAYS ---\")\n",
    "        day_result = identifier.find_best_clustering_method('day')\n",
    "        identifier.plot_analysis(day_result, 'day')\n",
    "        \n",
    "        print(\"\\n--- HOURS ---\")\n",
    "        hour_result = identifier.find_best_clustering_method('hour')\n",
    "        identifier.plot_analysis(hour_result, 'hour')\n",
    "        \n",
    "        # Generate config\n",
    "        config = identifier.generate_json_config(day_result, month_result, hour_result)\n",
    "        all_configs.append(config)\n",
    "    \n",
    "    # Save all configs\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\"groups\": all_configs}, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nConfiguration saved to {output_path}\")\n",
    "    return all_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb6a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_all_groups(df_processed, idx2pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pair2idx[(15, 'Doméstico')]\n",
    "df_processed[idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dba0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_all_df = pd.concat(df_processed)\n",
    "output_path = '../Data/input_ts.csv'\n",
    "seq_all_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed366bc",
   "metadata": {},
   "source": [
    "# Divide into groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5797361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def load_subgroups(json_path):\n",
    "    \"\"\"Load the subgroups JSON file.\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return data['groups']\n",
    "\n",
    "def divide_dataframes(df_list, idx2pair, json_path):\n",
    "    \"\"\"\n",
    "    Divide each DataFrame in df_list into subgroups based on the JSON configuration.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result : List with strcuture:\n",
    "                'df': pd.DataFrame,\n",
    "                'months': list,\n",
    "                'days': list,\n",
    "                'hours': list\n",
    "                \n",
    "    \"\"\"\n",
    "    # Load subgroup configurations\n",
    "    groups_config = load_subgroups(json_path)\n",
    "    \n",
    "    # Create a lookup dictionary for faster access\n",
    "    config_lookup = {}\n",
    "    for config in groups_config:\n",
    "        key = (int(config['calibre']), config['tipo_consumo'])\n",
    "        config_lookup[key] = config\n",
    "    \n",
    "\n",
    "    # Result \n",
    "    result = []\n",
    "    \n",
    "    # Process each DataFrame\n",
    "    for idx, df in enumerate(df_list):\n",
    "        # Get the (calibre, tipo_consumo) pair for this DataFrame\n",
    "        pair = idx2pair[idx]\n",
    "\n",
    "        # Get the configuration for this pair\n",
    "        if pair not in config_lookup:\n",
    "            print(f\"Warning: No configuration found for {pair}, skipping index {idx}\")\n",
    "            continue\n",
    "        \n",
    "        config = config_lookup[pair]\n",
    "        \n",
    "        # Get all subgroup combinations\n",
    "        month_groups = config['month_groups']\n",
    "        day_groups = config['day_groups']\n",
    "        hour_groups = config['hour_groups']\n",
    "        \n",
    "        # Create all combinations\n",
    "        subgroups = []\n",
    "        for months, days, hours in product(month_groups, day_groups, hour_groups): # Cartesian product\n",
    "            hours = list(map(int, hours))\n",
    "            \n",
    "            # Filter the DataFrame\n",
    "            mask = (\n",
    "                (df['month_name'].isin(months)) &\n",
    "                (df['day_name'].isin(days)) &\n",
    "                (df['hour'].isin(hours))\n",
    "            )\n",
    "            \n",
    "            subgroup_df = df[mask].copy()\n",
    "            \n",
    "            # Only add non-empty subgroups\n",
    "            if len(subgroup_df) > 0:\n",
    "                subgroups.append({\n",
    "                    'df': subgroup_df,\n",
    "                    'months': months,\n",
    "                    'days': days,\n",
    "                    'hours': hours\n",
    "                })\n",
    "        \n",
    "        result.append(subgroups)\n",
    "    \n",
    "    return result\n",
    "\n",
    "subgroups = divide_dataframes(df_processed, idx2pair, '../info/subgroups.json')\n",
    "\n",
    "# Access subgroups for a specific original DataFrame index\n",
    "for i, subgroup in enumerate(subgroups[0]):\n",
    "    print(f\"Subgroup {idx2pair[0]}:\")\n",
    "    print(f\"  Months: {subgroup['months']}\")\n",
    "    print(f\"  Days: {subgroup['days']}\")\n",
    "    print(f\"  Hours: {sorted(subgroup['hours'])}\")\n",
    "    print(f\"  DataFrame shape: {subgroup['df'].shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ad668",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd11b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create folder \"variables\" if it doesn't exist\n",
    "os.makedirs(\"../variables\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d889a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = (40, 'Comércio')\n",
    "idx = pair2idx[pair]\n",
    "for subgroup in subgroups[idx]:\n",
    "    print(f\"  Months: {subgroup['months']}\")\n",
    "    print(f\"  Days: {subgroup['days']}\")\n",
    "    print(f\"  Hours: {sorted(subgroup['hours'])}\")\n",
    "    print(f\"  DataFrame shape: {subgroup['df'].shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5c2e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Combine everything into one object\n",
    "data = {\n",
    "    \"idx2pair\": idx2pair,\n",
    "    \"pair2idx\": pair2idx,\n",
    "    \"subgroups\": subgroups,\n",
    "}\n",
    "\n",
    "# Save all as a single pickle file\n",
    "with open(\"../variables/variables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
